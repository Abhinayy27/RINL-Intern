# -*- coding: utf-8 -*-
"""BfPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AHM3fwa66zBUCpfL8VV5OJmHIXaxi4GG
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

path = '/content/drive/MyDrive/ML/bf3_data_jul21_dec21.xlsx'
df = pd.read_excel(path)

df.head()

df.isna().sum()

df.drop(['TOP_TEMP1', 'TOP_TEMP2','TOP_TEMP3','TOP_TEMP4','TOP_PRESS_1'], axis=1, inplace= True)
df.DATE_TIME = pd.to_datetime(df.DATE_TIME, format = "%d-%m-%y %H:%M")
df.head()

df.drop(df[df['SKIN_TEMP_AVG'].isna()].index, axis =0, inplace= True)
df.isna().sum()

sns.heatmap(df.corr(),cmap='RdBu')

for column in df.columns:
  if df.isna().sum()[column] !=0:
    plt.figure(figsize=(8, 5))
    sns.histplot(df[column], bins='fd')
    plt.title(f'Distribution of {column}')
    plt.xlabel(column)
    plt.ylabel('Frequency')
    plt.show()

from sklearn.impute import KNNImputer
df_new = df.copy(deep = True)
imputer = KNNImputer(n_neighbors = 6) # n_neighbors based on 1hr

df_new.loc[:, [column for column in df_new.columns if column != 'DATE_TIME']] = imputer.fit_transform(df_new.loc[:, [column for column in df_new.columns if column != 'DATE_TIME']])

# Rounding off the values to keep the column values discrete in nature
df_new.loc[:, ['STEAM_FLOW',]] = np.round(df_new.loc[:, ['STEAM_FLOW',]])

df_new.isna().sum()

df = df_new.copy(deep = True)

feature_cols = ['SKIN_TEMP_AVG_1', 'SKIN_TEMP_AVG_2', 'SKIN_TEMP_AVG_3', 'SKIN_TEMP_AVG_4']

for i in range(1, 5):
    df[feature_cols[i-1]] = df['SKIN_TEMP_AVG'].shift(-6*i)
df.head()

df.isna().sum()

df.dropna(inplace = True)

from sklearn.preprocessing import StandardScaler, MinMaxScaler

excluded_cols = feature_cols + ['DATE_TIME', 'SKIN_TEMP_AVG'] # Excluding the target features and the date time column
x = df[[x for x in df.columns if x not in excluded_cols]]
y = df['SKIN_TEMP_AVG']

scaler = StandardScaler()
scaled_features = scaler.fit_transform(x)
df_feat= pd.DataFrame(scaled_features, columns=df.columns[1:-5])

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error, accuracy_score

x_train, x_test, y_train, y_test = train_test_split(scaled_features,df['SKIN_TEMP_AVG'] , test_size=0.2, random_state = 42)
#x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.25, random_state = 42)

from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(x_train,y_train)
y_pred = model.predict(x_test)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print( mae,r2)

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
best_degree = 1
best_val_mae = float('inf')

for degree in range(1, 7):
    # Transform features to polynomial features
    poly = PolynomialFeatures(degree=degree)
    X_poly_train = poly.fit_transform(x_train)
    X_poly_val = poly.transform(x_val)

    # Fit the Linear Regression model
    model = LinearRegression()
    model.fit(X_poly_train, y_train)

    # Predict on the validation set
    y_val_pred = model.predict(X_poly_val)

    # Calculate validation metrics
    val_mae = mean_absolute_error(y_val, y_val_pred)'alidation MAE
    if val_mae < best_val_mae:
        best_val_mae = val_mae
        best_degree = degree

print(f"Best degree: {best_degree} with Validation MAE: {best_val_mae}")

# Final model training with the best degree
poly = PolynomialFeatures(degree=best_degree)
X_poly_train = poly.fit_transform(x_train)
X_poly_test = poly.transform(x_test)

model = LinearRegression()
model.fit(X_poly_train, y_train)

# Evaluate the final model on the test set
y_test_pred = model.predict(X_poly_test)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Test MAE: {test_mae}, Test R2: {test_r2}")

from sklearn.neighbors import KNeighborsRegressor
knn_params = {
    'regressor__n_neighbors' : [x for x in range(1, 10)],
    'regressor__weights' : ['uniform', 'distance'],
    'regressor__algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],
    'regressor__leaf_size' : [x for x in range(10, 51, 10)]
}

knn_pipeline = Pipeline([('scaler', MinMaxScaler()), ('regressor', KNeighborsRegressor())])
knn_grid_search = GridSearchCV(knn_pipeline, knn_params, cv = 5, scoring = ('r2', 'neg_mean_absolute_error', 'neg_mean_squared_error'), refit = 'neg_mean_squared_error', n_jobs = -1)
knn_grid_search.fit(x_train, y_train)

print(knn_grid_search.best_params_)
print(knn_grid_search.best_score_)

knn_model = Pipeline([('scaler', MinMaxScaler()), ('regressor', KNeighborsRegressor(n_neighbors=2, weights='distance', algorithm='auto', leaf_size=10, p=1))]).fit(x_train, y_train)
y_pred_knn = knn_model.predict(x_test)

print(mean_absolute_error(y_test, y_pred_knn))
print(r2_score(y_test, y_pred_knn))
print(mean_squared_error(y_test, y_pred_knn))

from sklearn.ensemble import RandomForestRegressor
rf_param_grid = {
    'regressor__n_estimators' : [x for x in range(100, 600, 100)],
    'regressor__max_depth' : [None, 10, 20, 30, 40, 50],
    'regressor__max_features' : ['auto', 'sqrt', 'log2', 0.3, 0.6, 0.9],
    'regressor__min_samples_split' : [2, 5, 10],
    'regressor__min_samples_leaf' : [1, 2, 4],
}

rf_pipeline = Pipeline([('scaler', StandardScaler()), ('regressor', RandomForestRegressor(random_state=42))])

rf_grid_search = GridSearchCV(rf_pipeline, rf_param_grid, cv = 5, scoring = ('r2', 'neg_mean_absolute_error', 'neg_mean_squared_error'), refit = 'neg_mean_squared_error', n_jobs = -1)
rf_grid_search.fit(x_train, y_train)

print(rf_grid_search.best_params_)
print(rf_grid_search.best_score_)

rf_model = Pipeline(steps = [('scaler', MinMaxScaler()), ('regressor', RandomForestRegressor(max_depth=30, max_features = 0.3, n_estimators=900, random_state=42))]).fit(X_train, y_train)
y_pred = rf_model.predict(X_test)
print(mean_absolute_error(y_test, y_pred))
print(r2_score(y_test, y_pred))
print(mean_squared_error(y_test, y_pred))

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

model = Sequential()
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(1, activation='linear'))
model.compile(optimizer= Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mean_absolute_error'])

early_stop = EarlyStopping(monitor= 'val_loss', mode= 'min', verbose=1, patience =25)

model.fit(x_train,y_train, epochs= 600, validation_split= 0.2, callbacks= [early_stop])

pred = model.predict(x_test)
mae = mean_absolute_error(y_test, pred)
print(f'Mean Absolute Error: {mae}')
r2 = r2_score(y_test, pred)
print(f'R-squared Score: {r2}')

for i in range(1,5):
    X_nxt_hr = x.copy(deep = True)
    if i-1 != 0:
        X_nxt_hr.loc[:, 'SKIN_TEMP_AVG'] = df['SKIN_TEMP_AVG_' + str(i-1)]
    else:
        X_nxt_hr.loc[:, 'SKIN_TEMP_AVG'] = df['SKIN_TEMP_AVG']

    y_nxt_hr = df['SKIN_TEMP_AVG_' + str(i)]

    X_train, X_test, y_train, y_test = train_test_split(X_nxt_hr, y_nxt_hr, test_size=0.2, random_state=42)

    print('r2: ' + str(r2_score(y_test, y_pred_knn)))
    print('mae: ' + str(mean_absolute_error(y_test, y_pred_knn)))
    print('mse: ' + str(mean_squared_error(y_test, y_pred_knn)))

